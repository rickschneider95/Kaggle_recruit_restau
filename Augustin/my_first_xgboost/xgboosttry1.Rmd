---
title: "Untitled"
author: "Augustin Baudoin"
date: "11/28/2018"
output: html_document
---

```{r setup, include=FALSE}
#temp <- read_csv("sample_submission.csv")
#temp2 <- read_csv("date_info.csv")


library(data.table)
library(mlr)

setcol <- c("age","workclass","fnlwgt","education","education-num", "marital-status","occupation", "relationship","race","sex","capital-gain","capital-loss","hours-per-week","native-country","target")
train <- read.table("adult_data.txt", header = F, sep = ",", col.names = setcol, na.strings = c(" ?"), stringsAsFactors = F)
test <- read.table("adult_test.txt",header = F,sep = ",",col.names = setcol,skip = 1, na.strings = c(" ?"),stringsAsFactors = F)
setDT(train)
setDT(test)

table(is.na(train))
sapply(train, function(x) sum(is.na(x))/length(x))*100
table(is.na(test))
sapply(test, function(x) sum(is.na(x))/length(x))*100

library(stringr)
test[,target := substr(target,start = 1,stop = nchar(target)-1)]

char_col <- colnames(train)[ sapply (test,is.character)]
char_col
for(i in char_col) set(train,j=i,value = str_trim(train[[i]],side = "left"))

for(i in char_col) set(test,j=i,value = str_trim(test[[i]],side = "left"))

#set all missing value as "Missing" 
train[is.na(train)] <- "Missing" 
test[is.na(test)] <- "Missing"

#using one hot encoding 
labels <- train$target 
ts_label <- test$target
new_tr <- model.matrix(~.+0,data = train[,-c("target"),with=F]) 
new_ts <- model.matrix(~.+0,data = test[,-c("target"),with=F])

colnames(new_tr)
colnames(new_ts)



new_tr <- new_tr[,-74]
class(new_tr)
#new_ts[,-"native.countryHoland-Netherlands"] <- 
setdiff(colnames(new_tr),colnames(new_ts))
#convert factor to numeric 
labels2 <- as.numeric(labels == ">50K")
ts_label2 <- as.numeric(ts_label == ">50K")
#View(labels)


library(xgboost)
?xgb.DMatrix
dtrain <- xgb.DMatrix(data = new_tr,label = labels2) 
dtest <- xgb.DMatrix(data = new_ts,label=ts_label2)
library(dplyr)




params <- list(booster = "gbtree", objective = "binary:logistic", eta=0.3, gamma=0, max_depth=6, min_child_weight=1, subsample=1, colsample_bytree=1)

xgbcv <- xgb.cv( params = params, data = dtrain, nrounds = 100, nfold = 5, showsd = T, stratified = T, print.every.n = 10, early.stop.round = 20, maximize = F)
xgbcv

xgb1 <- xgb.train (params = params, data = dtrain, nrounds = 79, watchlist = list(val=dtest,train=dtrain), print.every.n = 10, early.stop.round = 10, maximize = F , eval_metric = "error")
xgbpred <- predict(xgb1,dtest)
xgbpred <- ifelse (xgbpred > 0.5,1,0)
xgbpred

testcompare <- read.table("adult_test.txt",header = F,sep = ",",col.names = setcol,skip = 1, na.strings = c(" ?"),stringsAsFactors = F)
temp <- testcompare$target
temp <- as.numeric(temp == " >50K.")
temp
sum(as.numeric(xgbpred != temp))
sum(as.numeric(rbinom(16000,1,0.5) != rbinom(16000,1,0.5)))
#The above is how many times it'd be wrong if it were random, so clearly we are ok, altho we only predict correctly in 1-2063/16000 = 87% that's quite good 




new_tr3 <- model.matrix(~.+0,data = train[,-c("age"),with=F]) 
new_ts3 <- model.matrix(~.+0,data = test[,-c("age"),with=F])
setdiff(colnames(new_tr3),colnames(new_ts3))
setdiff(colnames(new_ts3),colnames(new_tr3))

View(new_ts3)
View(new_tr3)

new_tr3 <- new_tr3[,-73]
labels3 <- train$age
ts_labels3 <- test$age

dtrain3 <- xgb.DMatrix(data = new_tr3,label = labels3) 
dtest3 <- xgb.DMatrix(data = new_ts3,label=ts_labels3)

params3 <- list(booster = "gblinear", objective = "reg:linear", eta=0.3, gamma=0, max_depth=6, min_child_weight=1, subsample=1, colsample_bytree=1)

xgbcv3 <- xgb.cv( params = params3, data = dtrain3, nrounds = 100, nfold = 5, showsd = T, stratified = T, print.every.n = 10, early.stop.round = 20, maximize = F)
xgbcv3

xgb3 <- xgb.train (params = params, data = dtrain3, nrounds = 79, watchlist = list(val=dtest3,train=dtrain3), print.every.n = 10, early.stop.round = 10, maximize = F , eval_metric = "error")

xgbpred <- predict(xgb3,dtest3)

```
#Imma try to implement the xgboost now with the real estate dataset to see if we can fio







```{r}
real_estate <- read.csv("realestate.csv")
View(real_estate)
real_estate_train <- real_estate %>% slice(1:400) 
real_estate_test <- real_estate %>% slice(401:521)
#View(real_estate)
price_train <- real_estate_train$Price
price_test <- real_estate_test$Price
real_estate_train <- real_estate_train[,-2]
real_estate_train2 <- as.matrix(real_estate_train)
real_estate_test2 <- real_estate_test[,-2]
real_estate_test2 <- as.matrix(real_estate_test2)
dtrain2 <- xgb.DMatrix(data = real_estate_train2,label = price_train)
dtest2 <- xgb.DMatrix(data = real_estate_test2,label= price_test)


params <- list(booster = "gblinear", objective = "reg:linear", eta=0.3, gamma=0, max_depth=6, min_child_weight=1, subsample=1, colsample_bytree=1)

xgbcv <- xgb.cv( params = params, data = dtrain2, nrounds = 100, nfold = 5, showsd = T, stratified = T, print.every.n = 10, early.stop.round = 20, maximize = F)
xgbcv

xgb1 <- xgb.train (params = params, data = dtrain2, nrounds = 79, watchlist = list(val=dtest2,train=dtrain2), print.every.n = 10, early.stop.round = 10, maximize = F , eval_metric = "error")

xgbpred <- predict(xgb1,dtest2)

xgbpred
temp5 <- real_estate_test$Price
mean(abs(xgbpred - temp5))
library(MLmetrics)
RMSLE(xgbpred,temp5)
```

To recap, given a dataset, here are the steps we will take:
Dataset = TRAIN + TEST 

Let target be the variable you want to predict, namely the number of visitors in a day

target_train <- TRAIN$target
target_test <- TEST$target
dTRAIN <- as.matrix(TRAIN %>% select(-target))
dTEST <- as.matrix(TEST %>% select(-target))
dTRAIN <- xgb.DMatrix(data = dTRAIN,label = target_train)
dTEST <- xgb.DMatrix(data = dTEST,label= target_test)

params <- list(booster = "gblinear", objective = "reg:linear", eta=0.3, gamma=0, max_depth=6, min_child_weight=1, subsample=1, colsample_bytree=1)     #we can update later at will

xgbcv <- xgb.cv( params = params, data = dTRAIN, nrounds = 100, nfold = 5, showsd = T, stratified = T, print.every.n = 10, early.stop.round = 20, maximize = F)

xgb1 <- xgb.train (params = params, data = dTRAIN, nrounds = 79, watchlist = list(val=dTEST,train=dTRAIN), print.every.n = 10, early.stop.round = 10, maximize = F , eval_metric = "error")

xgbpred <- predict(xgb1,dTEST)

```{r}
our_data <- read.csv("data_1h_xgb.csv")
View(our_data)
TRAIN <- our_data %>% slice(1:300000)
TEST <- our_data %>% slice(300001:345120)
TRAIN <- TRAIN[,-1]
TEST <- TEST[,-1]

target_train <- TRAIN$visitors
target_test <- TEST$visitors

dTRAIN <- as.matrix(TRAIN %>% select(-visitors))
dTEST <- as.matrix(TEST %>% select(-visitors))
dTRAIN <- xgb.DMatrix(data = dTRAIN,label = target_train)
dTEST <- xgb.DMatrix(data = dTEST,label= target_test)

params <- list(booster = "gblinear", objective = "reg:linear", eta=0.3, gamma=0, max_depth=6, min_child_weight=1, subsample=1, colsample_bytree=1)     #we can update later at will

xgbcv <- xgb.cv( params = params, data = dTRAIN, nrounds = 100, nfold = 5, showsd = T, stratified = T, print.every.n = 10, early.stop.round = 20, maximize = F)

xgb1 <- xgb.train (params = params, data = dTRAIN, nrounds = 79, watchlist = list(val=dTEST,train=dTRAIN), print.every.n = 10, early.stop.round = 10, maximize = F , eval_metric = "error")

xgbpred <- predict(xgb1,dTEST)


```



